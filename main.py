import cv2
import mediapipe as mp
import sys
import time
import warnings
from collections import deque

# Import c√°c module x·ª≠ l√Ω c·ªßa d·ª± √°n
# ƒê·∫£m b·∫£o c·∫•u tr√∫c folder ƒë√∫ng nh∆∞ b·∫°n ƒë√£ upload
from utils.feature_extraction import extract_features
from utils.strings import ExpressionHandler
from utils.tts import TextToSpeech
from utils.model import ASLClassificationModel
from utils.post_processing import PredictionSmoother
from config import MODEL_NAME, MODEL_CONFIDENCE, SMOOTHING_WINDOW_SIZE, CONFIDENCE_THRESHOLD

import streamlit as st

# B·ªè qua c√°c c·∫£nh b√°o kh√¥ng c·∫ßn thi·∫øt
warnings.filterwarnings("ignore")

# ==========================================
# 1. C·∫§U H√åNH GIAO DI·ªÜN STREAMLIT
# ==========================================
st.set_page_config(page_title="ASL Recognition App", layout="wide")

st.markdown("""
    <style>
        .big-font {
            color: #e76f51 !important;
            font-size: 50px !important;
            font-weight: bold;
            border: 2px solid #fcbf49;
            border-radius: 10px;
            padding: 10px;
            text-align: center;
            background-color: #ffffff;
        }
        /* CƒÉn gi·ªØa video */
        div.stImage {
            text-align: center;
        }
        /* L√†m ƒë·∫πp thanh progress bar */
        .stProgress > div > div > div > div {
            background-color: #2a9d8f;
        }
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. H√ÄM LOAD MODEL (CACHE ƒê·ªÇ TƒÇNG T·ªêC)
# ==========================================
@st.cache_resource
def load_ai_model():
    """Load model m·ªôt l·∫ßn duy nh·∫•t ƒë·ªÉ tr√°nh lag khi reload"""
    print(f"ƒêang t·∫£i model t·ª´: models/{MODEL_NAME}...")
    return ASLClassificationModel.load_model(f"models/{MODEL_NAME}")

# Load model ngay khi v√†o app
try:
    model = load_ai_model()
    st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i model: {MODEL_NAME}")
except Exception as e:
    st.error(f"‚ùå L·ªói nghi√™m tr·ªçng: Kh√¥ng t√¨m th·∫•y model t·∫°i 'models/{MODEL_NAME}'")
    st.error(f"Chi ti·∫øt l·ªói: {e}")
    st.stop()

# ==========================================
# 3. SIDEBAR & C·∫§U H√åNH
# ==========================================
st.sidebar.title("üîß B·∫£ng ƒêi·ªÅu Khi·ªÉn")

# N√öT QUAN TR·ªåNG: B·∫¨T/T·∫ÆT CAMERA
run_camera = st.sidebar.checkbox("üì∑ B·∫≠t Camera", value=True)

# C·∫•u h√¨nh ƒë·ªô nh·∫°y AI
st.sidebar.markdown("---")
st.sidebar.subheader("üéõÔ∏è ƒê·ªô nh·∫°y AI")
detection_confidence = st.sidebar.slider("ƒê·ªô nh·∫°y ph√°t hi·ªán (Detection)", 0.0, 1.0, 0.7, 0.05, help="TƒÉng l√™n n·∫øu m√°y nh·∫≠n di·ªán nh·∫ßm nhi·ªÖu n·ªÅn l√† tay")
tracking_confidence = st.sidebar.slider("ƒê·ªô nh·∫°y theo d√µi (Tracking)", 0.0, 1.0, MODEL_CONFIDENCE, 0.05)
current_threshold = st.sidebar.slider("Ng∆∞·ª°ng ch·ªët ƒë√°p √°n (Threshold)", 0.0, 1.0, CONFIDENCE_THRESHOLD, 0.05, help="Ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ khi ƒë·ªô tin c·∫≠y v∆∞·ª£t qua m·ª©c n√†y")

# C·∫•u h√¨nh TTS (Gi·ªçng n√≥i)
st.sidebar.markdown("---")
st.sidebar.subheader("üîä C·∫•u h√¨nh Gi·ªçng n√≥i")
tts_enabled = st.sidebar.checkbox("B·∫≠t ƒë·ªçc k·∫øt qu·∫£", value=True)
tts_engine_choice = st.sidebar.selectbox("C√¥ng c·ª• ƒë·ªçc", ["gTTS (Google - Online, Ti·∫øng Vi·ªát hay)", "pyttsx3 (Offline - Nhanh)"], index=0)
min_interval = st.sidebar.slider("Kho·∫£ng c√°ch gi·ªØa c√°c l·∫ßn ƒë·ªçc (gi√¢y)", 1.0, 5.0, 2.5, 0.5)

# X·ª≠ l√Ω TTS Voice ID (n·∫øu d√πng pyttsx3)
tts_voice = None
if "pyttsx3" in tts_engine_choice:
    tts_voice = st.sidebar.text_input("Voice ID (pyttsx3 - T√πy ch·ªçn)", value="") or None

# --- Logic Kh·ªüi t·∫°o/Hu·ª∑ TTS Session ---
if 'tts' not in st.session_state:
    st.session_state.tts = None
    st.session_state.tts_engine = None

desired_engine = 'pyttsx3' if 'pyttsx3' in tts_engine_choice else 'gtts'

# N·∫øu b·∫≠t TTS nh∆∞ng ch∆∞a c√≥ object ho·∫∑c ƒë·ªïi engine -> T·∫°o m·ªõi
if tts_enabled:
    if st.session_state.tts is None or st.session_state.tts_engine != desired_engine:
        try:
            with st.spinner("ƒêang kh·ªüi t·∫°o gi·ªçng n√≥i..."):
                # L∆∞u √Ω: lang='vi' quan tr·ªçng cho gTTS
                st.session_state.tts = TextToSpeech(engine=desired_engine, lang='vi', voice=tts_voice)
                st.session_state.tts_engine = desired_engine
        except Exception as e:
            st.sidebar.error(f"L·ªói kh·ªüi t·∫°o TTS: {e}")
            tts_enabled = False
# N·∫øu t·∫Øt TTS m√† ƒëang c√≥ object -> H·ªßy
elif not tts_enabled and st.session_state.tts is not None:
    try:
        st.session_state.tts.stop()
    except:
        pass
    st.session_state.tts = None

# ==========================================
# 4. GIAO DI·ªÜN CH√çNH
# ==========================================
st.title("ü§ü Nh·∫≠n Di·ªán Ng√¥n Ng·ªØ K√Ω Hi·ªáu Vi·ªát Nam")

col1, col2 = st.columns([3, 2])

with col1:
    st.markdown("### üé• Camera Feed")
    video_placeholder = st.empty()

with col2:
    st.markdown("### üìù K·∫øt qu·∫£ D·ª± ƒëo√°n")
    prediction_placeholder = st.empty()
    
    st.markdown("#### ƒê·ªô tin c·∫≠y")
    confidence_bar = st.progress(0)
    confidence_text = st.empty()

    st.markdown("#### L·ªãch s·ª≠")
    history_placeholder = st.empty()

    # Khu v·ª±c hi·ªÉn th·ªã FPS
    st.markdown("---")
    fps_display = st.empty()
    status_text = st.empty()

# ==========================================
# 5. LOGIC X·ª¨ L√ù CAMERA (LOOP)
# ==========================================
if run_camera:
    # Kh·ªüi t·∫°o Mediapipe
    mp_holistic = mp.solutions.holistic
    mp_face_mesh = mp.solutions.face_mesh
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles

    # M·ªü Camera (Th·ª≠ index 0, n·∫øu l·ªói th·ª≠ 1)
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        st.error("Kh√¥ng th·ªÉ m·ªü Camera. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi.")
        st.stop()
    
    expression_handler = ExpressionHandler()
    # S·ª≠ d·ª•ng window size t·ª´ config ho·∫∑c hardcode nh·ªè h∆°n n·∫øu mu·ªën nhanh h∆°n
    smoother = PredictionSmoother(window_size=SMOOTHING_WINDOW_SIZE)
    
    prev_time = 0
    prediction_history = deque(maxlen=5)

    # Context Manager cho Mediapipe gi√∫p qu·∫£n l√Ω t√†i nguy√™n t·ªët h∆°n
    with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=detection_confidence,
            min_tracking_confidence=tracking_confidence) as face_mesh, \
         mp_hands.Hands(
            max_num_hands=2,
            min_detection_confidence=detection_confidence,
            min_tracking_confidence=tracking_confidence) as hands:

        while cap.isOpened() and run_camera:
            success, image = cap.read()
            if not success:
                st.warning("M·∫•t t√≠n hi·ªáu camera.")
                break

            # T√≠nh FPS
            curr_time = time.time()
            fps = 1 / (curr_time - prev_time) if (curr_time - prev_time) > 0 else 0
            prev_time = curr_time
            fps_display.metric("FPS (T·ªëc ƒë·ªô)", f"{int(fps)}")

            # Chu·∫©n b·ªã ·∫£nh cho Mediapipe (BGR -> RGB)
            image.flags.writeable = False
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # 1. Detect Faces & Hands
            face_results = face_mesh.process(image)
            hand_results = hands.process(image)

            # 2. V·∫Ω l·∫°i l√™n ·∫£nh (RGB -> BGR ƒë·ªÉ hi·ªÉn th·ªã opencv n·∫øu c·∫ßn, nh∆∞ng streamlit d√πng RGB c≈©ng ƒë∆∞·ª£c)
            # Tuy nhi√™n Mediapipe v·∫Ω ƒë·∫πp h∆°n tr√™n BGR g·ªëc r·ªìi convert l·∫°i sau, 
            # ·ªü ƒë√¢y ta v·∫Ω tr·ª±c ti·∫øp l√™n ·∫£nh RGB hi·ªán t·∫°i ƒë·ªÉ hi·ªÉn th·ªã lu√¥n
            image.flags.writeable = True
            
            # V·∫Ω Face
            if face_results.multi_face_landmarks:
                for face_landmarks in face_results.multi_face_landmarks:
                    mp_drawing.draw_landmarks(
                        image=image,
                        landmark_list=face_landmarks,
                        connections=mp_face_mesh.FACEMESH_TESSELATION,
                        landmark_drawing_spec=None,
                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)
                    )
            
            # V·∫Ω Hands
            if hand_results.multi_hand_landmarks:
                for hand_landmarks in hand_results.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(
                        image=image,
                        landmark_list=hand_landmarks,
                        connections=mp_hands.HAND_CONNECTIONS,
                        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),
                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)
                    )

            # ============================================================
            # 3. D·ª∞ ƒêO√ÅN C·ª¨ CH·ªà (ƒê√É S·ª¨A L·ªñI T·ª∞ ƒê·ªåC KHI KH√îNG C√ì TAY)
            # ============================================================
            
            # M·∫∑c ƒë·ªãnh l√† kh√¥ng c√≥ k·∫øt qu·∫£
            ui_text = "..."
            smoothed_confidence = 0.0
            
            # CH·ªà X·ª¨ L√ù KHI PH√ÅT HI·ªÜN C√ì B√ÄN TAY
            if hand_results.multi_hand_landmarks:
                try:
                    # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (Feature Extraction)
                    feature = extract_features(mp_hands, face_results, hand_results)
                    
                    # ƒê∆∞a v√†o model AI
                    label, confidence = model.predict_with_confidence(feature)
                    
                    # L√†m m∆∞·ª£t k·∫øt qu·∫£ (Smoothing)
                    smoother.add_prediction(label, confidence)
                    smoothed_label, smoothed_confidence = smoother.get_smoothed_prediction()

                    # Ch·ªâ hi·ªÉn th·ªã/ƒë·ªçc n·∫øu ƒë·ªô tin c·∫≠y v∆∞·ª£t ng∆∞·ª°ng (Threshold)
                    if smoothed_confidence >= current_threshold:
                        expression_handler.receive(smoothed_label)
                        ui_text = expression_handler.get_message()
                        
                        # C·∫≠p nh·∫≠t l·ªãch s·ª≠
                        if not prediction_history or prediction_history[-1] != ui_text:
                            prediction_history.append(ui_text)
                        
                        # ƒê·ªçc gi·ªçng n√≥i (TTS)
                        if tts_enabled and st.session_state.tts:
                            status_text.text(f"üîä ƒêang ƒë·ªçc: {ui_text}")
                            speech_text = expression_handler.get_speech_message()
                            st.session_state.tts.speak_if_allowed(speech_text, min_interval=min_interval)
                    else:
                        # C√≥ tay nh∆∞ng AI ch∆∞a ch·∫Øc ch·∫Øn
                        ui_text = "..." 
                        status_text.text("ü§î ƒêang ph√¢n t√≠ch...")

                except Exception as e:
                    print(f"L·ªói d·ª± ƒëo√°n: {e}")
                    status_text.text("‚ö†Ô∏è L·ªói x·ª≠ l√Ω AI")
            else:
                # KH√îNG C√ì TAY: Reset tr·∫°ng th√°i
                status_text.text("S·∫µn s√†ng. H√£y ƒë∆∞a tay v√†o camera.")
                # C√≥ th·ªÉ ch·ªçn reset b·ªô l√†m m∆∞·ª£t ƒë·ªÉ l·∫ßn sau ƒë∆∞a tay v√†o nh·∫≠n di·ªán nhanh h∆°n
                # smoother.clear() 

            # ============================================================
            # 4. C·∫¨P NH·∫¨T GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG
            # ============================================================
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ ch·ªØ to
            prediction_placeholder.markdown(f'<div class="big-font">{ui_text}</div>', unsafe_allow_html=True)
            
            # Hi·ªÉn th·ªã thanh ƒë·ªô tin c·∫≠y
            confidence_bar.progress(min(smoothed_confidence, 1.0))
            confidence_text.text(f"ƒê·ªô tin c·∫≠y: {round(smoothed_confidence * 100, 1)}%")
            
            # Hi·ªÉn th·ªã l·ªãch s·ª≠
            history_html = "<ul>" + "".join([f"<li>{item}</li>" for item in prediction_history]) + "</ul>"
            history_placeholder.markdown(history_html, unsafe_allow_html=True)

            # Hi·ªÉn th·ªã h√¨nh ·∫£nh camera
            video_placeholder.image(image, channels="RGB", use_column_width=True)

    # Gi·∫£i ph√≥ng camera khi tho√°t
    cap.release()
    cv2.destroyAllWindows()

else:
    # Giao di·ªán ch·ªù khi ch∆∞a b·∫≠t camera
    st.info("üëã Ch√†o m·ª´ng! H√£y t√≠ch v√†o √¥ 'üì∑ B·∫≠t Camera' ·ªü thanh b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.")
    video_placeholder.image("https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExbXp4Z3Bpbm94Z3Bpbm94Z3Bpbm94Z3Bpbm94Z3Bpbm94Z3Bpbm94Z3Bpbm94Z3Bpbm94ZCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o7TKUM3IgJBq2M3QA/giphy.gif", caption="Minh h·ªça ng√¥n ng·ªØ k√Ω hi·ªáu")
    prediction_placeholder.empty()
    fps_display.empty()